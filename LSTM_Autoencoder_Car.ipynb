{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Autoencoder_Car.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3tF9emDkBmAbKokXMaBI5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiguelMendozaG/CANproject/blob/6_features/LSTM_Autoencoder_Car.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nstTsL_j_1U_"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import optimizers\n",
        "DATA_SPLIT_PCT = 0.2\n",
        "SEED = 123\n",
        "max_size = 100\n",
        "n_features = 6\n",
        "lookback = max_size"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCg9AF4zASHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e460d5-c46a-4f16-9ed9-63b7fe1d43ec"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "model_dir = '/content/gdrive/My Drive/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHBnIT5LAjPI"
      },
      "source": [
        "car_model = \"Colab Notebooks/Car\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDDQ99N5B9PS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "94b08257-a5e2-467d-dadb-e110dccdb20c"
      },
      "source": [
        "model_dir + car_model"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/Car'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdCGfEXRAxyn"
      },
      "source": [
        "data = np.load(model_dir + car_model + '/recorrido.npy')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfBLLv9LMts8"
      },
      "source": [
        "def temporalize(X, y, lookback):\n",
        "    output_X = []\n",
        "    output_y = []\n",
        "    for i in range(len(X)-lookback-1):\n",
        "        t = []\n",
        "        for j in range(0,lookback):\n",
        "            # Gather past records upto the lookback period\n",
        "            t.append(X[[(i+j)], :])\n",
        "        output_X.append(t)\n",
        "        output_y.append(y[i+lookback])\n",
        "    return output_X, output_y\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp7sfbZz4SxV"
      },
      "source": [
        "def flatten(X):\n",
        "    '''\n",
        "    Flatten a 3D array.\n",
        "    \n",
        "    Input\n",
        "    X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
        "    \n",
        "    Output\n",
        "    flattened_X  A 2D array, sample x features.\n",
        "    '''\n",
        "    flattened_X = np.empty((X.shape[0], X.shape[2]) )  # sample x features array.\n",
        "    for i in range(X.shape[0]):\n",
        "      flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
        "    return(flattened_X)\n",
        "\n",
        "def scale(X, scaler):\n",
        "    '''\n",
        "    Scale 3D array.\n",
        "\n",
        "    Inputs\n",
        "    X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
        "    scaler       A scaler object, e.g., sklearn.preprocessing.StandardScaler, sklearn.preprocessing.normalize\n",
        "    \n",
        "    Output\n",
        "    X            Scaled 3D array.\n",
        "    '''\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i, :, :] = scaler.transform(X[i, :, :])\n",
        "        \n",
        "    return X"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0omkd8-SD3I3"
      },
      "source": [
        "X_train, X_test = train_test_split(np.array(data), test_size=DATA_SPLIT_PCT, random_state=SEED)\n",
        "X_train, X_validation = train_test_split(X_train, test_size=DATA_SPLIT_PCT, random_state=SEED)\n",
        "X_train_original_shape = X_train.shape\n",
        "X_test_original_shape = X_test.shape\n",
        "X_validation_original_shape = X_validation.shape"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdB0zT1cBNvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a8c48c-fe28-42c3-9143-b4b6b21de42e"
      },
      "source": [
        "X_train_reshape = X_train.reshape(X_train.shape[0], lookback, n_features)\n",
        "X_train_float = np.empty((X_train_reshape.shape[0], X_train_reshape.shape[1], X_train_reshape.shape[2]))\n",
        "X_train_float = np.array(X_train_reshape, dtype= np.float64)\n",
        "\n",
        "X_train_float_test = np.empty((X_train_reshape.shape[0], X_train_reshape.shape[1], X_train_reshape.shape[2]))\n",
        "X_train_float_test = np.array(X_train_reshape, dtype= np.float64)\n",
        "\n",
        "X_test_reshape = X_test.reshape(X_test.shape[0], lookback, n_features)\n",
        "X_test_float = np.empty((X_test_reshape.shape[0], X_test_reshape.shape[1], X_test_reshape.shape[2]))\n",
        "X_test_float = np.array(X_test_reshape, dtype= np.float64)\n",
        "\n",
        "X_validation_reshape = X_validation.reshape(X_validation.shape[0], lookback, n_features)\n",
        "X_validation_float = np.empty((X_validation_reshape.shape[0], X_validation_reshape.shape[1], X_validation_reshape.shape[2]))\n",
        "X_validation_float = np.array(X_validation_reshape, dtype= np.float64)\n",
        "\n",
        "print (X_train_reshape.shape)\n",
        "print (X_test_reshape.shape)\n",
        "print (X_validation_reshape.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24788, 100, 6)\n",
            "(7747, 100, 6)\n",
            "(6198, 100, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkrlpyqKLX9M"
      },
      "source": [
        "X_train_split = flatten(X_train_reshape)\n",
        "X_train_array = np.array(np.hsplit( X_train_split, 6))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWthUTniAN_9"
      },
      "source": [
        "# define min max scalers for each message\n",
        "scaler1 = MinMaxScaler()\n",
        "scaler2 = MinMaxScaler()\n",
        "scaler3 = MinMaxScaler()\n",
        "scaler4 = MinMaxScaler()\n",
        "scaler5 = MinMaxScaler()\n",
        "scaler6 = MinMaxScaler()\n",
        "\n",
        "X_train_float[:,:,0] = scaler1.fit_transform(X_train_reshape[:,:,0])\n",
        "X_train_float[:,:,1] = scaler2.fit_transform(X_train_reshape[:,:,1])\n",
        "X_train_float[:,:,2] = scaler3.fit_transform(X_train_reshape[:,:,2])\n",
        "X_train_float[:,:,3] = scaler4.fit_transform(X_train_reshape[:,:,3])\n",
        "X_train_float[:,:,4] = scaler5.fit_transform(X_train_reshape[:,:,4])\n",
        "X_train_float[:,:,5] = scaler6.fit_transform(X_train_reshape[:,:,5])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HNubvBCE9xI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e710746b-e120-425b-c325-43e84f78cda8"
      },
      "source": [
        "#Train statistics\n",
        "print (\"Msg 1\")\n",
        "print(X_train_float[:,:,0])\n",
        "print (X_train_float[:,:,0].min(), X_train_float[:,:,0].max())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Msg 1\n",
            "[[0.59090861 0.59090844 0.59090844 ... 0.59090852 0.68420848 0.684208  ]\n",
            " [0.59091065 0.59091065 0.59091065 ... 0.59090783 0.68420788 0.6842075 ]\n",
            " [0.59090835 0.59090835 0.59090852 ... 0.59090818 0.68420838 0.6842079 ]\n",
            " ...\n",
            " [0.59090826 0.59090826 0.59090826 ... 0.59090844 0.68420858 0.6842078 ]\n",
            " [0.59090826 0.59090844 0.59090844 ... 0.59090826 0.68420838 0.6842079 ]\n",
            " [0.59091186 0.59091177 0.59091177 ... 0.59091125 0.68421184 0.68421136]]\n",
            "0.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qOaxmqPFZyA"
      },
      "source": [
        "#transform test and validation with minmax statistics\n",
        "X_test_float[:,:,0] = scaler1.transform(X_test_reshape[:,:,0])\n",
        "X_test_float[:,:,1] = scaler2.transform(X_test_reshape[:,:,1])\n",
        "X_test_float[:,:,2] = scaler3.transform(X_test_reshape[:,:,2])\n",
        "X_test_float[:,:,3] = scaler4.transform(X_test_reshape[:,:,3])\n",
        "X_test_float[:,:,4] = scaler5.transform(X_test_reshape[:,:,4])\n",
        "X_test_float[:,:,5] = scaler6.transform(X_test_reshape[:,:,5])\n",
        "\n",
        "X_validation_float[:,:,0] = scaler1.transform(X_validation_reshape[:,:,0])\n",
        "X_validation_float[:,:,1] = scaler2.transform(X_validation_reshape[:,:,1])\n",
        "X_validation_float[:,:,2] = scaler3.transform(X_validation_reshape[:,:,2])\n",
        "X_validation_float[:,:,3] = scaler4.transform(X_validation_reshape[:,:,3])\n",
        "X_validation_float[:,:,4] = scaler5.transform(X_validation_reshape[:,:,4])\n",
        "X_validation_float[:,:,5] = scaler6.transform(X_validation_reshape[:,:,5])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY7CPt7GGNRn",
        "outputId": "60e7237f-19ec-4644-dc33-dc9d539b2ab3"
      },
      "source": [
        "#Test statistics\n",
        "print (\"Msg 1\")\n",
        "print(X_test_float[:,:,4])\n",
        "print (X_test_float[:,:,4].min(), X_test_float[:,:,4].max())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Msg 1\n",
            "[[0.74984747 0.74996949 0.75009152 ... 0.74996949 0.75009152 0.74972544]\n",
            " [0.84368517 0.8438072  0.84344112 ... 0.8438072  0.84344112 0.84356315]\n",
            " [0.84368517 0.8438072  0.84344112 ... 0.8438072  0.84344112 0.84356315]\n",
            " ...\n",
            " [0.8438072  0.84344112 0.84356315 ... 0.84344112 0.84356315 0.84368517]\n",
            " [0.8438072  0.84344112 0.84356315 ... 0.84344112 0.84356315 0.84368517]\n",
            " [0.84344112 0.84356315 0.84368517 ... 0.84356315 0.84368517 0.8438072 ]]\n",
            "0.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-wGc29HqBYD",
        "outputId": "fd0a813a-0fe9-428c-be5d-5445aa539e29"
      },
      "source": [
        "#Validation statistics\n",
        "num = 5\n",
        "print (\"Msg 1\")\n",
        "print(X_validation_float[:,:,num])\n",
        "print (X_validation_float[:,:,num].min(), X_validation_float[:,:,num].max())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Msg 1\n",
            "[[1.00448750e-15 1.20626613e-15 1.40452024e-15 ... 1.20626613e-15\n",
            "  1.40452024e-15 1.64859308e-15]\n",
            " [2.14285714e-01 2.14285714e-01 2.16269841e-01 ... 3.03571429e-01\n",
            "  3.05555556e-01 3.07539683e-01]\n",
            " [0.00000000e+00 2.37023804e-16 4.84621162e-16 ... 2.37023804e-16\n",
            "  4.84621162e-16 7.07546896e-16]\n",
            " ...\n",
            " [1.00448750e-15 1.20626613e-15 1.40452024e-15 ... 1.20626613e-15\n",
            "  1.40452024e-15 1.64859308e-15]\n",
            " [7.07546896e-16 1.00448750e-15 1.20626613e-15 ... 1.00448750e-15\n",
            "  1.20626613e-15 1.40452024e-15]\n",
            " [1.64859308e-15 1.99487693e-15 2.24599880e-15 ... 1.99487693e-15\n",
            "  2.24599880e-15 2.43720388e-15]]\n",
            "0.0 1.0000000000000004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wUWibR3nNQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f35bb05-9ac2-4890-acb4-bcdfc223e16a"
      },
      "source": [
        "timesteps = X_train_float.shape[1]\n",
        "n_features = X_train_float.shape[2]\n",
        "lr = 0.0001\n",
        "print(timesteps)\n",
        "print(n_features)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCvF3t0BmuE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa3b8ef-8d08-412b-f474-04ecac969c2e"
      },
      "source": [
        "lstm_autoencoder = Sequential()\n",
        "# Encoder\n",
        "lstm_autoencoder.add(LSTM(32, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))\n",
        "lstm_autoencoder.add(LSTM(16, activation='relu', return_sequences=False))\n",
        "lstm_autoencoder.add(RepeatVector(timesteps))\n",
        "# Decoder\n",
        "lstm_autoencoder.add(LSTM(16, activation='relu', return_sequences=True))\n",
        "lstm_autoencoder.add(LSTM(32, activation='relu', return_sequences=True))\n",
        "lstm_autoencoder.add(TimeDistributed(Dense(n_features)))\n",
        "\n",
        "lstm_autoencoder.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 100, 32)           4992      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 16)                3136      \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 100, 16)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100, 16)           2112      \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100, 32)           6272      \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 100, 6)            198       \n",
            "=================================================================\n",
            "Total params: 16,710\n",
            "Trainable params: 16,710\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edeFNYo2nCeI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7aebca71-6bc7-4699-f6c3-f39342e683fc"
      },
      "source": [
        "adam = optimizers.Adam(lr)\n",
        "lstm_autoencoder.compile(loss='mse', optimizer=adam)\n",
        "# fit model\n",
        "lstm_autoencoder_history = lstm_autoencoder.fit(X_train_float, X_train_float, validation_data=(X_validation_float, X_validation_float),\n",
        "                                                verbose=1, epochs=300, batch_size=100).history\n",
        "yhat = lstm_autoencoder.predict(X_test_float, verbose=0)\n",
        "\n",
        "plt.plot(lstm_autoencoder_history['loss'], linewidth=2, label='Train')\n",
        "plt.plot(lstm_autoencoder_history['val_loss'], linewidth=2, label='Valid')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "248/248 [==============================] - 42s 169ms/step - loss: 0.1947 - val_loss: 0.1452\n",
            "Epoch 2/300\n",
            "248/248 [==============================] - 41s 165ms/step - loss: 0.1228 - val_loss: 0.1080\n",
            "Epoch 3/300\n",
            "248/248 [==============================] - 41s 167ms/step - loss: 0.0799 - val_loss: 0.0534\n",
            "Epoch 4/300\n",
            "248/248 [==============================] - 42s 169ms/step - loss: 0.0421 - val_loss: 0.0369\n",
            "Epoch 5/300\n",
            "248/248 [==============================] - 42s 170ms/step - loss: 0.0329 - val_loss: 0.0310\n",
            "Epoch 6/300\n",
            "248/248 [==============================] - 42s 169ms/step - loss: 0.0276 - val_loss: 0.0258\n",
            "Epoch 7/300\n",
            "248/248 [==============================] - 43s 173ms/step - loss: 0.0231 - val_loss: 0.0221\n",
            "Epoch 8/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0202 - val_loss: 0.0194\n",
            "Epoch 9/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0176 - val_loss: 0.0167\n",
            "Epoch 10/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0151 - val_loss: 0.0145\n",
            "Epoch 11/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0133 - val_loss: 0.0130\n",
            "Epoch 12/300\n",
            "248/248 [==============================] - 43s 172ms/step - loss: 0.0120 - val_loss: 0.0119\n",
            "Epoch 13/300\n",
            "248/248 [==============================] - 43s 172ms/step - loss: 0.0111 - val_loss: 0.0111\n",
            "Epoch 14/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0105 - val_loss: 0.0105\n",
            "Epoch 15/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0099 - val_loss: 0.0100\n",
            "Epoch 16/300\n",
            "248/248 [==============================] - 43s 172ms/step - loss: 0.2056 - val_loss: 0.0182\n",
            "Epoch 17/300\n",
            "248/248 [==============================] - 43s 173ms/step - loss: 0.0125 - val_loss: 0.0112\n",
            "Epoch 18/300\n",
            "248/248 [==============================] - 43s 173ms/step - loss: 0.0103 - val_loss: 0.0105\n",
            "Epoch 19/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0099 - val_loss: 0.0101\n",
            "Epoch 20/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0096 - val_loss: 0.0098\n",
            "Epoch 21/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0093 - val_loss: 0.0096\n",
            "Epoch 22/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0091 - val_loss: 0.0094\n",
            "Epoch 23/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0090 - val_loss: 0.0092\n",
            "Epoch 24/300\n",
            "248/248 [==============================] - 43s 173ms/step - loss: 0.0088 - val_loss: 0.0090\n",
            "Epoch 25/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0086 - val_loss: 0.0089\n",
            "Epoch 26/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0085 - val_loss: 0.0087\n",
            "Epoch 27/300\n",
            "248/248 [==============================] - 43s 173ms/step - loss: 0.0083 - val_loss: 0.0086\n",
            "Epoch 28/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0082 - val_loss: 0.0084\n",
            "Epoch 29/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0081 - val_loss: 0.0083\n",
            "Epoch 30/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0079 - val_loss: 0.0082\n",
            "Epoch 31/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0078 - val_loss: 0.0080\n",
            "Epoch 32/300\n",
            "248/248 [==============================] - 43s 173ms/step - loss: 0.0077 - val_loss: 0.0079\n",
            "Epoch 33/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0075 - val_loss: 0.0078\n",
            "Epoch 34/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0074 - val_loss: 0.0076\n",
            "Epoch 35/300\n",
            "248/248 [==============================] - 43s 173ms/step - loss: 0.0073 - val_loss: 0.0075\n",
            "Epoch 36/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0072 - val_loss: 0.0074\n",
            "Epoch 37/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0071 - val_loss: 0.0073\n",
            "Epoch 38/300\n",
            "248/248 [==============================] - 43s 172ms/step - loss: 0.0069 - val_loss: 0.0071\n",
            "Epoch 39/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0068 - val_loss: 0.0070\n",
            "Epoch 40/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0067 - val_loss: 0.0069\n",
            "Epoch 41/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0066 - val_loss: 0.0068\n",
            "Epoch 42/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0065 - val_loss: 0.0067\n",
            "Epoch 43/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0063 - val_loss: 0.0065\n",
            "Epoch 44/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0062 - val_loss: 0.0064\n",
            "Epoch 45/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0061 - val_loss: 0.0063\n",
            "Epoch 46/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0060 - val_loss: 0.0062\n",
            "Epoch 47/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0059 - val_loss: 0.0061\n",
            "Epoch 48/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0058 - val_loss: 0.0060\n",
            "Epoch 49/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0057 - val_loss: 0.0059\n",
            "Epoch 50/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0056 - val_loss: 0.0058\n",
            "Epoch 51/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0055 - val_loss: 0.0057\n",
            "Epoch 52/300\n",
            "248/248 [==============================] - 43s 173ms/step - loss: 0.0055 - val_loss: 0.0056\n",
            "Epoch 53/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0054 - val_loss: 0.0055\n",
            "Epoch 54/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0053 - val_loss: 0.0055\n",
            "Epoch 55/300\n",
            "248/248 [==============================] - 43s 172ms/step - loss: 0.0052 - val_loss: 0.0054\n",
            "Epoch 56/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0051 - val_loss: 0.0053\n",
            "Epoch 57/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0050 - val_loss: 0.0052\n",
            "Epoch 58/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0050 - val_loss: 0.0051\n",
            "Epoch 59/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0049 - val_loss: 0.0051\n",
            "Epoch 60/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0048 - val_loss: 0.0050\n",
            "Epoch 61/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0048 - val_loss: 0.0049\n",
            "Epoch 62/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0047 - val_loss: 0.0049\n",
            "Epoch 63/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0046 - val_loss: 0.0048\n",
            "Epoch 64/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0046 - val_loss: 0.0047\n",
            "Epoch 65/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0045 - val_loss: 0.0047\n",
            "Epoch 66/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0045 - val_loss: 0.0046\n",
            "Epoch 67/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 68/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 69/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0043 - val_loss: 0.0045\n",
            "Epoch 70/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0042 - val_loss: 0.0044\n",
            "Epoch 71/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0042 - val_loss: 0.0043\n",
            "Epoch 72/300\n",
            "248/248 [==============================] - 43s 173ms/step - loss: 0.0041 - val_loss: 0.0043\n",
            "Epoch 73/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0041 - val_loss: 0.0042\n",
            "Epoch 74/300\n",
            "248/248 [==============================] - 43s 172ms/step - loss: 0.0040 - val_loss: 0.0041\n",
            "Epoch 75/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0040 - val_loss: 0.0041\n",
            "Epoch 76/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0039 - val_loss: 0.0040\n",
            "Epoch 77/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0038 - val_loss: 0.0040\n",
            "Epoch 78/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0038 - val_loss: 0.0039\n",
            "Epoch 79/300\n",
            "248/248 [==============================] - 43s 174ms/step - loss: 0.0037 - val_loss: 0.0038\n",
            "Epoch 80/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0037 - val_loss: 0.0038\n",
            "Epoch 81/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0036 - val_loss: 0.0037\n",
            "Epoch 82/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0036 - val_loss: 0.0036\n",
            "Epoch 83/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0035 - val_loss: 0.0036\n",
            "Epoch 84/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0035 - val_loss: 0.0036\n",
            "Epoch 85/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 86/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0034 - val_loss: 0.0035\n",
            "Epoch 87/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0033 - val_loss: 0.0035\n",
            "Epoch 88/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0033 - val_loss: 0.0034\n",
            "Epoch 89/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0033 - val_loss: 0.0034\n",
            "Epoch 90/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0032 - val_loss: 0.0033\n",
            "Epoch 91/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0032 - val_loss: 0.0033\n",
            "Epoch 92/300\n",
            "248/248 [==============================] - 46s 184ms/step - loss: 0.0032 - val_loss: 0.0033\n",
            "Epoch 93/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0031 - val_loss: 0.0032\n",
            "Epoch 94/300\n",
            "248/248 [==============================] - 45s 183ms/step - loss: 0.0031 - val_loss: 0.0032\n",
            "Epoch 95/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0031 - val_loss: 0.0031\n",
            "Epoch 96/300\n",
            "248/248 [==============================] - 45s 183ms/step - loss: 0.0030 - val_loss: 0.0031\n",
            "Epoch 97/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0030 - val_loss: 0.0031\n",
            "Epoch 98/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0030 - val_loss: 0.0031\n",
            "Epoch 99/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0029 - val_loss: 0.0031\n",
            "Epoch 100/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0029 - val_loss: 0.0030\n",
            "Epoch 101/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0029 - val_loss: 0.0030\n",
            "Epoch 102/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 103/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0028 - val_loss: 0.0029\n",
            "Epoch 104/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0028 - val_loss: 0.0030\n",
            "Epoch 105/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0028 - val_loss: 0.0029\n",
            "Epoch 106/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0028 - val_loss: 0.0029\n",
            "Epoch 107/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 108/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0028 - val_loss: 0.0029\n",
            "Epoch 109/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0027 - val_loss: 0.0028\n",
            "Epoch 110/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0027 - val_loss: 0.0028\n",
            "Epoch 111/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0027 - val_loss: 0.0028\n",
            "Epoch 112/300\n",
            "248/248 [==============================] - 46s 186ms/step - loss: 0.0027 - val_loss: 0.0028\n",
            "Epoch 113/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0027 - val_loss: 0.0028\n",
            "Epoch 114/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 115/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 116/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 117/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0026 - val_loss: 0.0027\n",
            "Epoch 118/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0026 - val_loss: 0.0028\n",
            "Epoch 119/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 120/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 121/300\n",
            "248/248 [==============================] - 45s 183ms/step - loss: 0.0025 - val_loss: 0.0026\n",
            "Epoch 122/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0025 - val_loss: 0.0026\n",
            "Epoch 123/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0025 - val_loss: 0.0026\n",
            "Epoch 124/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 125/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 126/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0024 - val_loss: 0.0026\n",
            "Epoch 127/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0024 - val_loss: 0.0025\n",
            "Epoch 128/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0024 - val_loss: 0.0025\n",
            "Epoch 129/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0024 - val_loss: 0.0025\n",
            "Epoch 130/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 131/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 132/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 133/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 134/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 135/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 136/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 137/300\n",
            "248/248 [==============================] - 46s 184ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 138/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 139/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 140/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 141/300\n",
            "248/248 [==============================] - 45s 183ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 142/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 143/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 144/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 145/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 146/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 147/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 148/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 149/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 150/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 151/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 152/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 153/300\n",
            "248/248 [==============================] - 46s 186ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 154/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 155/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 156/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0020 - val_loss: 0.0021\n",
            "Epoch 157/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0020 - val_loss: 0.0021\n",
            "Epoch 158/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0020 - val_loss: 0.0021\n",
            "Epoch 159/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 160/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0020 - val_loss: 0.0021\n",
            "Epoch 161/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 162/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 163/300\n",
            "248/248 [==============================] - 45s 179ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 164/300\n",
            "248/248 [==============================] - 46s 184ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 165/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 166/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 167/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 168/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 169/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 170/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 171/300\n",
            "248/248 [==============================] - 45s 183ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 172/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 173/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 174/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 175/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 176/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 177/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 178/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 179/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 180/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 181/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 182/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 183/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 184/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 185/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 186/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 187/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 188/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 189/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 190/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 191/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 192/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 193/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 194/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 195/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 196/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 197/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 198/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 199/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 200/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 201/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 202/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 203/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 204/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 205/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 206/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 207/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 208/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 209/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 210/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 211/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 212/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 213/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 214/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 215/300\n",
            "248/248 [==============================] - 47s 190ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 216/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 217/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 218/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 219/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 220/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 221/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 222/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 223/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 224/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 225/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 226/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 227/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 228/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 229/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 230/300\n",
            "248/248 [==============================] - 45s 179ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 231/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 232/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 233/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 234/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 235/300\n",
            "248/248 [==============================] - 46s 187ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 236/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 237/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 238/300\n",
            "248/248 [==============================] - 46s 184ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 239/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 240/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 241/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 242/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 243/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 244/300\n",
            "248/248 [==============================] - 45s 183ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 245/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 246/300\n",
            "248/248 [==============================] - 45s 183ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 247/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 248/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 249/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 250/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 251/300\n",
            "248/248 [==============================] - 45s 183ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 252/300\n",
            "248/248 [==============================] - 45s 183ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 253/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 254/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 255/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 256/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 257/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 258/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 259/300\n",
            "248/248 [==============================] - 45s 183ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 260/300\n",
            "248/248 [==============================] - 46s 184ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 261/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 262/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 263/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 264/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 265/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 266/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 267/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 268/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 269/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 270/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 271/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 272/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 273/300\n",
            "248/248 [==============================] - 45s 183ms/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 274/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 275/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 276/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 277/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 278/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 279/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 280/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 281/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 282/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 283/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 284/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 285/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 286/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 287/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 288/300\n",
            "248/248 [==============================] - 43s 175ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 289/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 290/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 291/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 292/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 293/300\n",
            "248/248 [==============================] - 44s 178ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 294/300\n",
            "248/248 [==============================] - 45s 182ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 295/300\n",
            "248/248 [==============================] - 45s 180ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 296/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 297/300\n",
            "248/248 [==============================] - 45s 181ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 298/300\n",
            "248/248 [==============================] - 44s 179ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 299/300\n",
            "248/248 [==============================] - 44s 177ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 300/300\n",
            "248/248 [==============================] - 44s 176ms/step - loss: 0.0012 - val_loss: 0.0012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6ba7fdca49fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_autoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_autoencoder_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_autoencoder_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyp24qz3ppNy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}